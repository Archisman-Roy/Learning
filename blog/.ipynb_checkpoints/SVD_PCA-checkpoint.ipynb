{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work can be considered as notes from Steve burton's [lecture series](https://www.youtube.com/watch?v=gXbThCXjZFM&list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv) on Youtube.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular value decomposition aka SVDs is a data reduction technique which is used to convert a higher dimensinal data to lower dimensional data without losing too much information. Infact SVD is actually used to compute principal components in almost every library which has a PCA module. \n",
    "\n",
    "Applications of SVD is common in big tech companies like Google (pagerank), Facebook (Face detection), Amazon (recommendations) etc. This is primarily because of SVD's simplicity, interpretabilty and scalability for large datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to put together the basic SVD equation which is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \\large \\mathbf{X} = \\mathbf{U}\\Sigma\\mathbf{V}^\\top where$$ \n",
    "\n",
    "\n",
    "\\begin{matrix}X =  [x_1 & x_2 & .. & x_m] \\end{matrix} \n",
    "\n",
    "$$ \\ x_1, \\ x_2 \\ ... \\ x_m \\ are \\ column \\ vectors \\ and \\ each \\ vector \\ has \\ n \\ elements $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use image data (MNIST) to construct the matrix X. Each column vector would represent a MNIST number and will have 784 elements (28*28 pixels). In this case, m is \"Number of images\" and n is \"Number of pixels in each image\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "U_{n,n} = \n",
    "\\begin{pmatrix}\n",
    ". & . & \\cdots & . \\\\\n",
    "u_{1} & u_{2} & \\cdots & u_{n} \\\\\n",
    ". & . & \\cdots & . \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Sigma_{n,m} = \n",
    "\\begin{pmatrix}\n",
    "\\sigma_{1} & . &  . &  . \\\\\n",
    ". & \\sigma_{2} & . \\\\\n",
    ". & . &  . &  .\\\\\n",
    ". & . & . & \\sigma_{n}\\\\\n",
    ". & . &  . &  . \\\\\n",
    ". & . &  . &  .\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "V_{m,m}^\\top = \n",
    "\\begin{pmatrix}\n",
    ". & . & \\cdots & . \\\\\n",
    "v_{1} & v_{2} & \\cdots & v_{m} \\\\\n",
    ". & . & \\cdots & . \\\\\n",
    "\\end{pmatrix}^\\top\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try to understand the output matrices U, Sigma and V. \n",
    "\n",
    "U and V are unitary matrices and Sigma is a diagonal matrix. Shape of matrix U is nxn i.e. shape of column vectors of U is same as shape of column vectors of X. Column vectors of U are arranged in descending order by their ability to explain variance in matrix X. The diagonal elements in Sigma matrix captures the value of how much variance is explained by the column vectors of matrix U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BTW - Orthogonal matrices have column vectors who are all perpendicular to each other (Dot product between 2 different column vector will be zero). Orthonormal matrices are orthogonal matrices with an additional property that each column vectors have “Unit Length” or have length = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, note that the following properties hold true for U, V and Sigma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{U} \\mathbf{U}^\\top = \\mathbf{I}_{n,n}  $$ \n",
    "\n",
    "$$ \\mathbf{V} \\mathbf{V}^\\top = \\mathbf{I}_{m,m}  $$ \n",
    "\n",
    "$$ where \\  \\mathbf{I} \\ is \\ an \\ identity \\ matrix  $$\n",
    "\n",
    "\n",
    "$$ \\Sigma \\ Diagonal: \\sigma_{1} \\geqslant \\sigma_{2} \\geqslant \\sigma_{3} \\geqslant ... \\geqslant \\sigma_{n} \\geqslant 0   $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, U is aka left singular vectors,\n",
    "V is aka right singular vectors, \n",
    "and sigma is aka singular values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an interpretation for the right singular vector or V matrix as well. In case of MNIST data, column vectors of V^T gives the exact mixture of weights to get back column vectors of X using U and sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, U, V and sigma are guranteed to exist for all X and it's also unique. Now we can represent the SVD multiplication in the following manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "X_{n,m} =\n",
    "\\left[\n",
    "\\sigma_{1} \n",
    "*\n",
    "\\begin{pmatrix}\n",
    ". \\\\\n",
    "u_{1} \\\\\n",
    ". \\\\\n",
    "\\end{pmatrix}_{n,1}\n",
    "* \n",
    "\\begin{pmatrix}\n",
    ". & v_{1} & . \\\\\n",
    "\\end{pmatrix}_{1,m}\n",
    "\\right]\n",
    "+\n",
    "\\left[\n",
    "\\sigma_{2} \n",
    "*\n",
    "\\begin{pmatrix}\n",
    ". \\\\\n",
    "u_{2} \\\\\n",
    ". \\\\\n",
    "\\end{pmatrix}_{n,1}\n",
    "* \n",
    "\\begin{pmatrix}\n",
    ". & v_{2} & . \\\\\n",
    "\\end{pmatrix}_{1,m}\n",
    "\\right]\n",
    "\\cdots\n",
    "+\n",
    "\\left[\n",
    "\\sigma_{r} \n",
    "*\n",
    "\\begin{pmatrix}\n",
    ". \\\\\n",
    "u_{r} \\\\\n",
    ". \\\\\n",
    "\\end{pmatrix}_{n,1}\n",
    "* \n",
    "\\begin{pmatrix}\n",
    ". & v_{r} & . \\\\\n",
    "\\end{pmatrix}_{1,m}\n",
    "\\right]\n",
    "\\cdots\n",
    "+\n",
    "\\left[\n",
    "\\sigma_{m} \n",
    "*\n",
    "\\begin{pmatrix}\n",
    ". \\\\\n",
    "u_{m} \\\\\n",
    ". \\\\\n",
    "\\end{pmatrix}_{n,1}\n",
    "* \n",
    "\\begin{pmatrix}\n",
    ". & v_{m} & . \\\\\n",
    "\\end{pmatrix}_{1,m}\n",
    "\\right]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the aforementioned summation can be truncated at \"r\" instead of \"m\" which will give us an approximation of the input matrix X i.e. summing all the elements till \"m\" will give us exact X, however stopping the summation at \"r\" will give us approx X with the same shape n*m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As next steps we will define a correlation matrix aka co-variance matrix and see the connection to PCA and SVD. A correlation matrix is defined as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Given X_{n,m}, \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "X^T . X =\n",
    "\\begin{pmatrix}\n",
    "X_{1}^T.X_{1} & X_{1}^T.X_{2} &  ... &  X_{1}^T.X_{m} \\\\\n",
    "X_{2}^T.X_{1} & X_{2}^T.X_{2} &  ... &  X_{2}^T.X_{m} \\\\\n",
    "... & ... &  ... &  X_{1}^T.X_{m} \\\\\n",
    "X_{m}^T.X_{1} & X_{m}^T.X_{2} &  ... &  X_{m}^T.X_{m} \\\\\n",
    "\\end{pmatrix}_{m,m}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aforementioned matrix can be represented differently using the SVD decomposition as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "X^T . X = V.\\Sigma^T.U^T.U.\\Sigma.V^T\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Rightarrow  X^T . X = V.\\Sigma^T.I.\\Sigma.V^T\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Rightarrow  X^T . X .V = V.\\Sigma^T.\\Sigma.V^T . V\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Rightarrow  X^T . X .V = V.\\Sigma^T.\\Sigma.I\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Rightarrow  X^T . X .V = V.\\Sigma^2\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where V is the eigen vector and Sigma are eigen values. This equation shows the link between PCA and SVD computation used in all ML modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
